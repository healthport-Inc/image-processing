{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13542632",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "\n",
    "\n",
    "# from utils import CustomDataset, GANLoss, Vgg19\n",
    "# from generator import AttU_Net\n",
    "# from discriminator import Discriminator\n",
    "\n",
    "from PIL import Image\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8495596e",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c891a4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_path = \"../OCR_data/doc_unet/original/1_1.jpg\"\n",
    "\n",
    "img_path = \"/srv/data/hamin/OCR/doc_unet/original/1_1.jpg\"\n",
    "\n",
    "im = Image.open(img_path)\n",
    "#im = im.resize((num_pixels_y, num_pixels_x))\n",
    "im = np.array(im)\n",
    "\n",
    "print(f\"0: {im.shape[0] // 2}, 1: {im.shape[1] // 2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15710053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_base_path = \"../OCR_data/doc_unet/original/\"\n",
    "# crop_base_path = \"../OCR_data/doc_unet/crop/\"\n",
    "# scan_base_path = \"../OCR_data/doc_unet/scan/\"\n",
    "\n",
    "\n",
    "orig_base_path = \"/srv/data/hamin/OCR/doc_unet/original/\"\n",
    "crop_base_path = \"/srv/data/hamin/OCR/doc_unet/crop/\"\n",
    "scan_base_path = \"/srv/data/hamin/OCR/doc_unet/scan/\"\n",
    "\n",
    "\n",
    "num_pixels_x, num_pixels_y, num_channels = 2016//16, 1512//16, 3\n",
    "\n",
    "orig_images = np.zeros((len(os.listdir(orig_base_path)), num_pixels_x, num_pixels_y, num_channels), dtype=np.uint8)\n",
    "crop_images = np.zeros((len(os.listdir(orig_base_path)), num_pixels_x, num_pixels_y, num_channels), dtype=np.uint8)\n",
    "scan_images = np.zeros((len(os.listdir(orig_base_path)), num_pixels_x, num_pixels_y, num_channels), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cace68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for count, im_path in enumerate(tqdm(sorted(os.listdir(orig_base_path)))):\n",
    "\n",
    "    im = Image.open(f\"{orig_base_path}/{im_path}\")\n",
    "    im = im.resize((num_pixels_y, num_pixels_x))\n",
    "    im = np.array(im)\n",
    "\n",
    "    orig_images[count] = im\n",
    "    #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc321b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d006910",
   "metadata": {},
   "outputs": [],
   "source": [
    "for count, im_path in enumerate(tqdm(sorted(os.listdir(crop_base_path)))):\n",
    "\n",
    "    im = Image.open(f\"{crop_base_path}/{im_path}\")\n",
    "    im = im.resize((num_pixels_y, num_pixels_x))\n",
    "    im = np.array(im)\n",
    "\n",
    "    crop_images[count] = im\n",
    "    #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec525d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Rename scan folder to make everything sorted\n",
    "# for im_path in sorted(os.listdir(scan_base_path)):\n",
    "#     os.rename(f\"{scan_base_path}{im_path}\", f\"{scan_base_path}{im_path[:-4]}_1.png\")\n",
    "\n",
    "for count, im_path in enumerate(tqdm(sorted(os.listdir(scan_base_path)))):\n",
    "\n",
    "    cur_count = count*2\n",
    "    im = Image.open(f\"{scan_base_path}/{im_path}\")\n",
    "    im = im.convert('RGB')\n",
    "    im = im.resize((num_pixels_y, num_pixels_x))\n",
    "    im = np.array(im)\n",
    "\n",
    "    scan_images[cur_count] = im\n",
    "    scan_images[cur_count+1] = im\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6075444",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,3, figsize=(10,7))\n",
    "\n",
    "i = -2\n",
    "\n",
    "cur_orig = orig_images[i]\n",
    "cur_crop = crop_images[i]\n",
    "cur_scan = scan_images[i]\n",
    "\n",
    "\n",
    "axarr[0].imshow(cur_orig)\n",
    "axarr[0].title.set_text('Original')\n",
    "\n",
    "axarr[1].imshow(cur_crop)\n",
    "axarr[1].title.set_text('Cropped')\n",
    "\n",
    "axarr[2].imshow(cur_scan)\n",
    "axarr[2].title.set_text('scanned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ab937",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/srv/data/hamin/OCR/doc_unet/\"\n",
    "# with open(f\"{base_path}/orig.npy\", 'wb') as f:\n",
    "#     np.save(f, orig_images)\n",
    "    \n",
    "# with open(f\"{base_path}/crop.npy\", 'wb') as f:\n",
    "#     np.save(f, crop_images)\n",
    "    \n",
    "# with open(f\"{base_path}/scan.npy\", 'wb') as f:\n",
    "#     np.save(f, scan_images)\n",
    "    \n",
    "    \n",
    "# with open(f\"{base_path}/orig.npy\", 'rb') as f:\n",
    "#     orig_images = np.load(f)\n",
    "    \n",
    "# with open(f\"{base_path}/crop.npy\", 'rb') as f:\n",
    "#     crop_images = np.load(f)\n",
    "    \n",
    "# with open(f\"{base_path}/scan.npy\", 'rb') as f:\n",
    "#     scan_images = np.load(f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc34b81",
   "metadata": {},
   "source": [
    "# Phase 1: get cropped image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe5ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(orig_images, crop_images, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b105c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,2, figsize=(10,7))\n",
    "\n",
    "i = -6\n",
    "\n",
    "cur_orig = x_train[i]\n",
    "cur_crop = y_train[i]\n",
    "#cur_scan = scan_images[i]\n",
    "\n",
    "\n",
    "axarr[0].imshow(cur_orig)\n",
    "axarr[0].title.set_text('Original')\n",
    "\n",
    "axarr[1].imshow(cur_crop)\n",
    "axarr[1].title.set_text('Cropped')\n",
    "\n",
    "# axarr[2].imshow(cur_scan)\n",
    "# axarr[2].title.set_text('scanned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc71032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, transform_input=None, transform_output=None ):\n",
    "        self.transform_input = transform_input\n",
    "        self.transform_output = transform_output\n",
    "\n",
    "        self.data = np.transpose(x_data, (0, 3, 1, 2))\n",
    "        self.targets = np.transpose(y_data, (0, 3, 1, 2))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.targets.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        x = Image.fromarray(self.data[index].astype(np.uint8).transpose(1, 2, 0))\n",
    "        x = self.transform_input(x)\n",
    "\n",
    "        y = Image.fromarray(self.targets[index].astype(np.uint8).transpose(1, 2, 0))      \n",
    "        y = self.transform_output(y)\n",
    "\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcadbabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    #transforms.RandomAffine(degrees=[-10,10], translate=[0.00,0.15], scale=[0.7,1.00], shear=1, fill=(0,0,0)),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean = [ 0.5, 0.5, 0.5 ], std = [ 0.5, 0.5, 0.5 ]),\n",
    "    #transforms.RandomErasing(scale=[0.05, 0.08], ratio=[0.02,0.05], p=0.5),\n",
    "])\n",
    "\n",
    "transform_output = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean = [ 0.5, 0.5, 0.5 ], std = [ 0.5, 0.5, 0.5 ]),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean = [ 0.5, 0.5, 0.5 ], std = [ 0.5, 0.5, 0.5 ]),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f8655",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDataset(x_train, y_train,\n",
    "                          transform_input=transform_train, \n",
    "                          transform_output=transform_output, \n",
    "                         )\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, num_workers=16)\n",
    "\n",
    "test_set = CustomDataset(x_test, y_test, \n",
    "                          transform_input=transform_test, \n",
    "                          transform_output=transform_output, \n",
    "                        )\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4087c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity test\n",
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "    \n",
    "    \n",
    "for x,y in trainloader:\n",
    "    break\n",
    "\n",
    "    \n",
    "unorm = UnNormalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])\n",
    "image_x = unorm(x)\n",
    "image_y = unorm(y)\n",
    "\n",
    "image_x = torch.permute(image_x, (0, 2,3, 1)).numpy()\n",
    "image_y = torch.permute(image_y, (0, 2,3, 1)).numpy()\n",
    "\n",
    "\n",
    "f, axarr = plt.subplots(1,2, figsize=(10,7))\n",
    "i = 0\n",
    "cur_orig = image_x[i]\n",
    "cur_crop = image_y[i]\n",
    "#cur_scan = scan_images[i]\n",
    "\n",
    "\n",
    "axarr[0].imshow(cur_orig)\n",
    "axarr[0].title.set_text('Original')\n",
    "\n",
    "axarr[1].imshow(cur_crop)\n",
    "axarr[1].title.set_text('Cropped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6036c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4268512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.u2net import U2NETP, U2NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf6984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = U2NETP()\n",
    "# generator = U2NET()\n",
    "\n",
    "generator = nn.DataParallel(generator)\n",
    "generator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbc757",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # A bunch of convolutions one after another\n",
    "        model = [   nn.Conv2d(input_nc, 64, 4, stride=2, padding=1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        model += [  nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "                    nn.InstanceNorm2d(128), \n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        model += [  nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "                    nn.InstanceNorm2d(256), \n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        model += [  nn.Conv2d(256, 512, 4, padding=1),\n",
    "                    nn.InstanceNorm2d(512), \n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        # FCN classification layer\n",
    "        model += [nn.Conv2d(512, 1, 4, padding=1)]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (0.1**0.5)*(torch.randn(x.shape[0], x.shape[1], x.shape[2], x.shape[3])).cuda()\n",
    "        x =  self.model(x)\n",
    "        # Average pooling and flatten\n",
    "        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fceb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "discriminator = Discriminator()\n",
    "\n",
    "discriminator = nn.DataParallel(discriminator)\n",
    "discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142fe103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e654957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg19\n",
    "from collections import namedtuple\n",
    "\n",
    "class Vgg19(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vgg19, self).__init__()\n",
    "        features = list(vgg19(pretrained = True).features)[:36]\n",
    "        self.features = nn.ModuleList(features).eval()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        results = []\n",
    "        for ii,model in enumerate(self.features):\n",
    "            x = model(x)\n",
    "            if ii in {2,7,12,21,30}:\n",
    "                results.append(x)\n",
    "        return results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8745e30a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perceptual_model = Vgg19()\n",
    "\n",
    "perceptual_model = nn.DataParallel(perceptual_model)\n",
    "perceptual_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c683b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b81a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb24e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
    "        if use_lsgan:\n",
    "            self.loss = nn.MSELoss()\n",
    "        else:\n",
    "            self.loss = nn.BCELoss()\n",
    "\n",
    "    def get_target_tensor(self, input, target_is_real):\n",
    "        if target_is_real:\n",
    "            target_tensor = self.real_label\n",
    "        else:\n",
    "            target_tensor = self.fake_label\n",
    "        return target_tensor.expand_as(input)\n",
    "\n",
    "    def __call__(self, input, target_is_real):\n",
    "        target_tensor = self.get_target_tensor(input, target_is_real)\n",
    "        return self.loss(input, target_tensor)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e399a5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "\n",
    "\n",
    "criterion_g = torch.nn.MSELoss()\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(), lr=lr,\n",
    "                             betas=(0.5, 0.999)\n",
    "                            )\n",
    "\n",
    "\n",
    "criterion_d = GANLoss().to(device)\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=lr,\n",
    "                             betas=(0.5, 0.999)\n",
    "                            )\n",
    "\n",
    "criterion_p = torch.nn.MSELoss()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total = len(train_set)\n",
    "train_batches = len(trainloader)\n",
    "\n",
    "test_total = len(test_set)\n",
    "test_baches = len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5591667",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5000\n",
    "\n",
    "patience = 0    # Bad epoch counter\n",
    "best_loss = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f521c1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    \n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    for x, y in trainloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        #forward\n",
    "        fake, _, _, _, _, _, _ = generator(x)\n",
    "        \n",
    "        \n",
    "        #########################\n",
    "        # (1) Calculate Perceptual Loss\n",
    "        #########################\n",
    "\n",
    "        fake_feature = perceptual_model(fake)\n",
    "        real_feature = perceptual_model(y)\n",
    "\n",
    "        loss_p = 0\n",
    "        for i in range(5):\n",
    "            loss_p += criterion_p(fake_feature[i], real_feature[i])        \n",
    "        \n",
    "        \n",
    "        ##########################\n",
    "        # (2) Update Discriminator\n",
    "        ##########################\n",
    "        \n",
    "        #Train with fake\n",
    "        optimizer_d.zero_grad()\n",
    "        pred_fake = discriminator(fake)\n",
    "        loss_d_fake = criterion_d(pred_fake, False)\n",
    "        \n",
    "        #Train with real\n",
    "        pred_real = discriminator(y)\n",
    "        loss_d_real = criterion_d(pred_real, True)\n",
    "        \n",
    "        #Average and update\n",
    "        loss_d_total = (loss_d_fake + loss_d_real) * 0.5\n",
    "        loss_d_total.backward(retain_graph=True)\n",
    "        optimizer_d.step()\n",
    "        \n",
    "        \n",
    "        ###########################\n",
    "        # (3) Update Generator\n",
    "        ###########################\n",
    "        optimizer_g.zero_grad()\n",
    "        \n",
    "        #Get Discriminator Loss\n",
    "        pred_fake = discriminator(fake)\n",
    "        loss_gan = criterion_d(pred_fake, True)\n",
    "        \n",
    "        \n",
    "        #Get Generator Loss\n",
    "        loss_g = criterion_g(fake, y)\n",
    "        \n",
    "        loss_total = loss_gan + loss_g + loss_p\n",
    "        loss_total.backward(retain_graph=True)\n",
    "        optimizer_g.step()\n",
    "\n",
    "        train_loss += loss_total.item()\n",
    "\n",
    "    train_loss = train_loss / train_batches\n",
    "\n",
    "    #scheduler.step(1.)\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print('[%2d / %d] train_loss: %.5f  ' % (epoch+1, num_epochs , train_loss), end = ' ')\n",
    "        \n",
    "    generator.eval()\n",
    "    \n",
    "    \n",
    "    ### test acc ###\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            x, y = data[0].to(device), data[1].to(device)\n",
    "            outputs, _,_,_,_,_,_ = generator(x)\n",
    "            \n",
    "            loss = criterion_g(outputs, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            \n",
    "        test_loss = test_loss / test_baches\n",
    "        \n",
    "        if(test_loss < best_loss):\n",
    "            #torch.save(generator.state_dict(), path_checkpoint)\n",
    "            #torch.save(discriminator.state_dict(), path_checkpoint2)\n",
    "            best_loss = test_loss\n",
    "        print('test_loss: %.5f -- Best loss: %.5f --- %.2f seconds' %(test_loss, best_loss, (end_time-start_time)))    \n",
    "        \n",
    "        \n",
    "#     unorm = UnNormalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])\n",
    "#     x = unorm(x)\n",
    "#     y_hat = unorm(outputs)\n",
    "#     y = unorm(y)\n",
    "    y_hat = outputs\n",
    "\n",
    "    x = torch.permute(x.detach().cpu(), (0, 2,3, 1)).numpy()\n",
    "    y_hat = torch.permute(y_hat.detach().cpu(), (0, 2,3, 1)).numpy()\n",
    "    y = torch.permute(y.detach().cpu(), (0, 2,3, 1)).numpy()\n",
    "\n",
    "    #for i in range(len(x)):\n",
    "    \n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        f, axarr = plt.subplots(1,3, figsize=(10,7))\n",
    "        i = 0\n",
    "        cur_orig = x[i]\n",
    "        cur_pred = y_hat[i]\n",
    "        cur_crop = y[i]\n",
    "        #cur_scan = scan_images[i]\n",
    "\n",
    "\n",
    "        axarr[0].imshow(cur_orig)\n",
    "        axarr[0].title.set_text('Original')\n",
    "\n",
    "        axarr[1].imshow(cur_pred)\n",
    "        axarr[1].title.set_text('Predicted')\n",
    "\n",
    "        axarr[2].imshow(cur_crop)\n",
    "        axarr[2].title.set_text('real')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cf7cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try doing CV2 algin first, then run the models?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
